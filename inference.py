#!/usr/bin/env python3
"""
RCH-StackBot-3.8B - Final Production Version
Svensk Fullstack Utvecklingsassistent
"""

import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import argparse
import time
from datetime import datetime

class RCHStackBot:
    def __init__(self, model_path="./models/RCH-StackBot-3.8B"):
        print("üöÄ RCH-StackBot startar...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üì± Enhet: {self.device}")
        
        # Ladda modell
        self._load_model(model_path)
        print("‚úÖ RCH-StackBot √§r redo!\n")
    
    def _load_model(self, model_path):
        """Ladda modell och tokenizer"""
        print("üì• Laddar tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print("üì• Laddar basmodell...")
        self.model = AutoModelForCausalLM.from_pretrained(
            "microsoft/Phi-3-mini-4k-instruct",
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # F√∂rs√∂k ladda fine-tunad version
        adapter_config_path = os.path.join(model_path, "adapter_config.json")
        if os.path.exists(adapter_config_path):
            try:
                from peft import PeftModel
                print("üì• Laddar fine-tunad modell...")
                self.model = PeftModel.from_pretrained(self.model, model_path)
                print("‚úÖ Fine-tunad modell laddad!")
                self.is_finetuned = True
            except Exception as e:
                print(f"‚ö†Ô∏è Kunde inte ladda fine-tunad modell: {e}")
                print("üìù Anv√§nder basmodell")
                self.is_finetuned = False
        else:
            print("üìù Anv√§nder basmodell (ingen fine-tuning hittades)")
            self.is_finetuned = False

    def generate_response(self, prompt, max_length=200, temperature=0.7):
        """Generera svar fr√•n modellen"""
        # Enkel prompt-formatering f√∂r Phi-3
        formatted_prompt = f"<|user|>\n{prompt}<|end|>\n<|assistant|>\n"
        
        # Tokenisera
        inputs = self.tokenizer(
            formatted_prompt, 
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(self.device)
        
        # Generera svar
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.15,
                no_repeat_ngram_size=3,
                top_p=0.9,
                top_k=50
            )
        
        # Dekoda och rensa svaret
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extrahera assistant-delen
        if "<|assistant|>" in response:
            response = response.split("<|assistant|>")[-1].strip()
        
        # Rensa artefakter
        if "<|end|>" in response:
            response = response.split("<|end|>")[0].strip()
        
        return response

    def interactive_chat(self):
        """Interaktiv chat-session"""
        print("ü§ñ RCH-StackBot Interactive Chat")
        print("=" * 50)
        print("üéØ Specialomr√•den:")
        print("   ‚Ä¢ Frontend: React, Vue, Angular, HTML, CSS")
        print("   ‚Ä¢ Backend: Node.js, Express, Python, databases")  
        print("   ‚Ä¢ JavaScript: ES6+, TypeScript, async/await")
        print("   ‚Ä¢ DevOps: Docker, deployment, API:er")
        print("   ‚Ä¢ Allm√§nt: Kodning, best practices, debugging")
        
        model_status = "Fine-tunad" if self.is_finetuned else "Basmodell"
        print(f"üìä Status: {model_status}")
        
        print("\nüí¨ Kommandon:")
        print("   ‚Ä¢ 'exit' - Avsluta")
        print("   ‚Ä¢ 'help' - Visa hj√§lp")
        print("   ‚Ä¢ 'exempel' - Visa exempel-fr√•gor")
        print("=" * 50)
        
        conversation_count = 0
        start_time = time.time()
        
        while True:
            try:
                user_input = input(f"\nüßë‚Äçüíª [{conversation_count + 1}] Du: ").strip()
                
                if user_input.lower() in ['exit', 'quit', 'avsluta', 'bye']:
                    duration = time.time() - start_time
                    print(f"\nüìä Session-statistik:")
                    print(f"   Fr√•gor: {conversation_count}")
                    print(f"   Tid: {duration:.1f} sekunder")
                    print("üëã Tack f√∂r att du anv√§nde RCH-StackBot!")
                    break
                
                if not user_input:
                    continue
                
                if user_input.lower() in ['help', 'hj√§lp']:
                    self._show_help()
                    continue
                
                if user_input.lower() in ['exempel', 'examples']:
                    self._show_examples()
                    continue
                
                # Generera svar
                print("ü§ñ T√§nker...", end="", flush=True)
                start_gen = time.time()
                
                response = self.generate_response(user_input)
                
                gen_time = time.time() - start_gen
                print(f"\rü§ñ Bot [{gen_time:.1f}s]: {response}\n")
                
                conversation_count += 1
                
            except KeyboardInterrupt:
                print("\n\nüëã Chat avbruten. Hej d√•!")
                break
            except Exception as e:
                print(f"\n‚ùå Fel uppstod: {e}")
                print("F√∂rs√∂k igen...")

    def _show_help(self):
        """Visa hj√§lp-information"""
        print("\nüÜò RCH-StackBot Hj√§lp")
        print("=" * 30)
        print("üìö B√§sta s√§tt att st√§lla fr√•gor:")
        print("‚Ä¢ Var specifik: 'Hur skapar jag en React useState hook?'")
        print("‚Ä¢ N√§mn teknik: 'I Node.js, hur l√§ser jag en fil?'")
        print("‚Ä¢ Be om exempel: 'Visa kod f√∂r en Express route'")
        print("‚Ä¢ Fels√∂kning: 'Varf√∂r f√•r jag CORS-fel i min API?'")
        
    def _show_examples(self):
        """Visa exempel-fr√•gor"""
        print("\nüí° Exempel-fr√•gor:")
        print("=" * 20)
        examples = [
            "Hur skapar jag en React functional component?",
            "Vad √§r skillnaden mellan == och === i JavaScript?",
            "Hur s√§tter jag upp en Express server med CORS?",
            "F√∂rklara async/await vs Promises",
            "Hur anv√§nder jag CSS Grid f√∂r layout?",
            "Vad √§r skillnaden mellan localStorage och sessionStorage?",
            "Hur g√∂r jag en fetch-request i JavaScript?",
            "F√∂rklara vad REST API:er √§r"
        ]
        
        for i, example in enumerate(examples, 1):
            print(f"{i}. {example}")

    def demo_mode(self):
        """Demo med exempel-fr√•gor"""
        print("üéØ RCH-StackBot Demo Mode")
        print("=" * 30)
        
        demo_questions = [
            "Vad √§r React?",
            "Hur skapar jag en variabel i JavaScript?", 
            "F√∂rklara vad en REST API √§r",
            "Vad √§r skillnaden mellan let och const?"
        ]
        
        for i, question in enumerate(demo_questions, 1):
            print(f"\n{i}. üìù {question}")
            response = self.generate_response(question, max_length=150)
            print(f"ü§ñ {response}")
            print("-" * 60)
        
        print(f"\nüí° K√∂r med --interactive f√∂r chat-mode")

def main():
    parser = argparse.ArgumentParser(
        description="RCH-StackBot - Svensk Fullstack Utvecklingsassistent",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exempel:
  python stackbot.py                                    # Demo mode
  python stackbot.py --interactive                      # Chat mode  
  python stackbot.py --prompt "Vad √§r React?"          # Enskild fr√•ga
  python stackbot.py --prompt "Kod exempel" --temp 0.3  # L√§gre kreativitet
        """
    )
    
    parser.add_argument("--prompt", "-p", type=str, help="Enskild fr√•ga att st√§lla")
    parser.add_argument("--interactive", "-i", action="store_true", help="Starta interaktiv chat")
    parser.add_argument("--model_path", "-m", default="./models/RCH-StackBot-3.8B", help="S√∂kv√§g till modell")
    parser.add_argument("--max_length", "-l", type=int, default=200, help="Max tokens i svar")
    parser.add_argument("--temperature", "-t", type=float, default=0.7, help="Kreativitet (0.1-1.0)")
    
    args = parser.parse_args()
    
    try:
        # Skapa bot
        bot = RCHStackBot(model_path=args.model_path)
        
        if args.prompt:
            # Enskild fr√•ga
            print(f"üìù Fr√•ga: {args.prompt}")
            print("ü§ñ Svar:")
            response = bot.generate_response(
                args.prompt, 
                max_length=args.max_length, 
                temperature=args.temperature
            )
            print(response)
            
        elif args.interactive:
            # Interaktiv mode
            bot.interactive_chat()
            
        else:
            # Demo mode
            bot.demo_mode()
    
    except KeyboardInterrupt:
        print("\nüëã Programmet avbr√∂ts. Hej d√•!")
    except Exception as e:
        print(f"\n‚ùå Ett fel uppstod: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()